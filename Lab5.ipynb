{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingketan9/BigDataLabs/blob/main/Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null ### Installing Java"
      ],
      "metadata": {
        "id": "E-asmSoMg7Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz #### Installing Spark and Hadoop on the worker node\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "ECTPETpHg7ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark #### Creating a Sparksession"
      ],
      "metadata": {
        "id": "-tckWWupg9PS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "917ed7b3-467a-4c75-ef1d-f2cf2b3acbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7afab0087850>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7f7cb137b5e5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the following dataframe\n",
        "\n",
        " df1 = spark.createDataFrame([(1, 144.5, 5.9, 33, 'M'),(2, 167.2, 5.4, 45, 'M'),(3, 124.1, 5.2, 23, 'F'),(4, 140.4, 5.2, 34, 'M'),(5, 133.2, 5.7, 54, 'F'),(6, 114.1, 4.8, 23, 'F'),\n",
        "(7, 129.2, 5.3, 2, 'M'),], ['id', 'weight', 'height', 'age', 'gender'])"
      ],
      "metadata": {
        "id": "ZDReS_6cgwED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jYu5OrJw4vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9638c7-07a6-42e6-e569-e91eae41ce8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+\n",
            "| id|weight|height|age|gender|\n",
            "+---+------+------+---+------+\n",
            "|  1| 144.5|   5.9| 33|     M|\n",
            "|  2| 167.2|   5.4| 45|     M|\n",
            "|  3| 124.1|   5.2| 23|     F|\n",
            "|  4| 140.4|   5.2| 34|     M|\n",
            "|  5| 133.2|   5.7| 54|     F|\n",
            "|  6| 114.1|   4.8| 23|     F|\n",
            "|  7| 129.2|   5.3|  2|     M|\n",
            "+---+------+------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"weight\", DoubleType(), True),\n",
        "    StructField(\"height\", DoubleType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (1, 144.5, 5.9, 33, 'M'),\n",
        "    (2, 167.2, 5.4, 45, 'M'),\n",
        "    (3, 124.1, 5.2, 23, 'F'),\n",
        "    (4, 140.4, 5.2, 34, 'M'),\n",
        "    (5, 133.2, 5.7, 54, 'F'),\n",
        "    (6, 114.1, 4.8, 23, 'F'),\n",
        "    (7, 129.2, 5.3, 2, 'M')\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "df1.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create another dataframe\n",
        "\n",
        "\n",
        " df2 = spark.createDataFrame([(3, 'HR', 2500),(4, 'HR', 2700),(2, 'Marketing', 1300),(5, 'Marketing', 1900),(1, 'Technical', 2100),(7, 'Technical', 2500),(8, 'Admin', 2600),\n",
        " ], ['id', 'Department', 'salary'])"
      ],
      "metadata": {
        "id": "gL6wYu5mhON-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema2 = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"Department\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data2 = [\n",
        "    (3, 'HR', 2500),\n",
        "    (4, 'HR', 2700),\n",
        "    (2, 'Marketing', 1300),\n",
        "    (5, 'Marketing', 1900),\n",
        "    (1, 'Technical', 2100),\n",
        "    (7, 'Technical', 2500),\n",
        "    (8, 'Admin', 2600)\n",
        "]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, schema=schema2)\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "B0MQ2RZr7p1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02fbfaf-9b61-4a9d-e1cc-147e8173383a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "| id|Department|salary|\n",
            "+---+----------+------+\n",
            "|  3|        HR|  2500|\n",
            "|  4|        HR|  2700|\n",
            "|  2| Marketing|  1300|\n",
            "|  5| Marketing|  1900|\n",
            "|  1| Technical|  2100|\n",
            "|  7| Technical|  2500|\n",
            "|  8|     Admin|  2600|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform various join operations on these two dataframes. Also explain what the join operations do."
      ],
      "metadata": {
        "id": "XZU2B7V5haj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inner_join = df1.join(df2, on=['id'], how='inner')\n",
        "\n",
        "left_join = df1.join(df2, on=['id'], how='left')\n",
        "\n",
        "right_join = df1.join(df2, on=['id'], how='right')\n",
        "\n",
        "outer_join = df1.join(df2, on=['id'], how='outer')"
      ],
      "metadata": {
        "id": "L8WLYtAy8n5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform an inner join of these two dataframes."
      ],
      "metadata": {
        "id": "sb8riA5Fh2i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inner_join.show()"
      ],
      "metadata": {
        "id": "fkA4booe8AFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9df30e-064d-4961-a3a5-7c6886340a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+----------+------+\n",
            "| id|weight|height|age|gender|Department|salary|\n",
            "+---+------+------+---+------+----------+------+\n",
            "|  1| 144.5|   5.9| 33|     M| Technical|  2100|\n",
            "|  3| 124.1|   5.2| 23|     F|        HR|  2500|\n",
            "|  5| 133.2|   5.7| 54|     F| Marketing|  1900|\n",
            "|  4| 140.4|   5.2| 34|     M|        HR|  2700|\n",
            "|  7| 129.2|   5.3|  2|     M| Technical|  2500|\n",
            "|  2| 167.2|   5.4| 45|     M| Marketing|  1300|\n",
            "+---+------+------+---+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform an outer join of these two dataframes."
      ],
      "metadata": {
        "id": "6V-RQf46h_oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outer_join.show()"
      ],
      "metadata": {
        "id": "VotR-waR8NYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f525d6-b616-4b6e-de72-9b6e8ca81a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+----+------+----------+------+\n",
            "| id|weight|height| age|gender|Department|salary|\n",
            "+---+------+------+----+------+----------+------+\n",
            "|  1| 144.5|   5.9|  33|     M| Technical|  2100|\n",
            "|  6| 114.1|   4.8|  23|     F|      null|  null|\n",
            "|  3| 124.1|   5.2|  23|     F|        HR|  2500|\n",
            "|  5| 133.2|   5.7|  54|     F| Marketing|  1900|\n",
            "|  4| 140.4|   5.2|  34|     M|        HR|  2700|\n",
            "|  8|  null|  null|null|  null|     Admin|  2600|\n",
            "|  7| 129.2|   5.3|   2|     M| Technical|  2500|\n",
            "|  2| 167.2|   5.4|  45|     M| Marketing|  1300|\n",
            "+---+------+------+----+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a leftouter join of these two dataframes."
      ],
      "metadata": {
        "id": "4s__bShNiHDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_join.show()"
      ],
      "metadata": {
        "id": "iKIKl_lBiG3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44abf0d0-0d7c-4218-d416-9821cdfb65bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+----------+------+\n",
            "| id|weight|height|age|gender|Department|salary|\n",
            "+---+------+------+---+------+----------+------+\n",
            "|  1| 144.5|   5.9| 33|     M| Technical|  2100|\n",
            "|  6| 114.1|   4.8| 23|     F|      null|  null|\n",
            "|  3| 124.1|   5.2| 23|     F|        HR|  2500|\n",
            "|  5| 133.2|   5.7| 54|     F| Marketing|  1900|\n",
            "|  4| 140.4|   5.2| 34|     M|        HR|  2700|\n",
            "|  7| 129.2|   5.3|  2|     M| Technical|  2500|\n",
            "|  2| 167.2|   5.4| 45|     M| Marketing|  1300|\n",
            "+---+------+------+---+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a rightouter join of these two dataframes."
      ],
      "metadata": {
        "id": "Wc-xNhOOiNUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "right_join.show()"
      ],
      "metadata": {
        "id": "s8HL5mSd8TwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0d2fdb-0b02-42a8-babf-1b8f2b031b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+----+------+----------+------+\n",
            "| id|weight|height| age|gender|Department|salary|\n",
            "+---+------+------+----+------+----------+------+\n",
            "|  1| 144.5|   5.9|  33|     M| Technical|  2100|\n",
            "|  3| 124.1|   5.2|  23|     F|        HR|  2500|\n",
            "|  5| 133.2|   5.7|  54|     F| Marketing|  1900|\n",
            "|  4| 140.4|   5.2|  34|     M|        HR|  2700|\n",
            "|  8|  null|  null|null|  null|     Admin|  2600|\n",
            "|  7| 129.2|   5.3|   2|     M| Technical|  2500|\n",
            "|  2| 167.2|   5.4|  45|     M| Marketing|  1300|\n",
            "+---+------+------+----+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a leftsemi join of these two dataframes.\n"
      ],
      "metadata": {
        "id": "FJNFizWni49W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_semi_join = df1.join(df2, on=['id'], how='left_semi')\n",
        "\n",
        "left_semi_join.show()"
      ],
      "metadata": {
        "id": "gOk3tfYX8zna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1994ec8b-b899-40d5-b941-c509ef26b2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+\n",
            "| id|weight|height|age|gender|\n",
            "+---+------+------+---+------+\n",
            "|  1| 144.5|   5.9| 33|     M|\n",
            "|  3| 124.1|   5.2| 23|     F|\n",
            "|  5| 133.2|   5.7| 54|     F|\n",
            "|  4| 140.4|   5.2| 34|     M|\n",
            "|  7| 129.2|   5.3|  2|     M|\n",
            "|  2| 167.2|   5.4| 45|     M|\n",
            "+---+------+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a leftanti join of these two dataframes."
      ],
      "metadata": {
        "id": "adFTVmmdi9pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_anti_join = df1.join(df2, on=['id'], how='left_anti')\n",
        "\n",
        "left_anti_join.show()"
      ],
      "metadata": {
        "id": "44amJNEm9AjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c080220-cb11-463d-88ca-5c280cc13fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+\n",
            "| id|weight|height|age|gender|\n",
            "+---+------+------+---+------+\n",
            "|  6| 114.1|   4.8| 23|     F|\n",
            "+---+------+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create the dataframe\n",
        "\n",
        "\n",
        "df = spark.createDataFrame([(1, 144.5, 5.9, 33, 'M'),(2, 167.2, 5.4, 45, 'M'),(3, 124.1, 5.2, 23, 'F'),(4, 144.5, 5.9, 33, 'M'),(5, 133.2, 5.7, 54, 'F'),(3, 124.1, 5.2, 23, 'F'),\n",
        "(5, 129.2, 5.3, 42, 'M'),], ['id', 'weight', 'height', 'age', 'gender'])"
      ],
      "metadata": {
        "id": "uW_E5ChRk8X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema3 = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"weight\", DoubleType(), True),\n",
        "    StructField(\"height\", DoubleType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True)\n",
        "])\n",
        "\n",
        "data3 = [\n",
        "    (1, 144.5, 5.9, 33, 'M'),\n",
        "    (2, 167.2, 5.4, 45, 'M'),\n",
        "    (3, 124.1, 5.2, 23, 'F'),\n",
        "    (4, 144.5, 5.9, 33, 'M'),\n",
        "    (5, 133.2, 5.7, 54, 'F'),\n",
        "    (3, 124.1, 5.2, 23, 'F'),\n",
        "    (5, 129.2, 5.3, 42, 'M')\n",
        "]\n",
        "\n",
        "df3 = spark.createDataFrame(data3, schema=schema3)\n",
        "\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "sjGUTPsDlOYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f91bd4-f754-4f8f-d1e2-93dd34dfac82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+\n",
            "| id|weight|height|age|gender|\n",
            "+---+------+------+---+------+\n",
            "|  1| 144.5|   5.9| 33|     M|\n",
            "|  2| 167.2|   5.4| 45|     M|\n",
            "|  3| 124.1|   5.2| 23|     F|\n",
            "|  4| 144.5|   5.9| 33|     M|\n",
            "|  5| 133.2|   5.7| 54|     F|\n",
            "|  3| 124.1|   5.2| 23|     F|\n",
            "|  5| 129.2|   5.3| 42|     M|\n",
            "+---+------+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop duplicate rows"
      ],
      "metadata": {
        "id": "Ls25p6JDlUAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3_no_duplicates = df3.dropDuplicates()\n",
        "\n",
        "df3_no_duplicates.show()"
      ],
      "metadata": {
        "id": "4slb1K4z9grJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280ae094-1140-40fb-955e-f095a8f79161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+\n",
            "| id|weight|height|age|gender|\n",
            "+---+------+------+---+------+\n",
            "|  1| 144.5|   5.9| 33|     M|\n",
            "|  5| 133.2|   5.7| 54|     F|\n",
            "|  5| 129.2|   5.3| 42|     M|\n",
            "|  3| 124.1|   5.2| 23|     F|\n",
            "|  4| 144.5|   5.9| 33|     M|\n",
            "|  2| 167.2|   5.4| 45|     M|\n",
            "+---+------+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop rows which have the same data but different IDs"
      ],
      "metadata": {
        "id": "2yoBIrUjlbxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3_no_duplicate = df3.dropDuplicates(subset=[col for col in df3.columns if col != 'id'])\n",
        "\n",
        "df3_no_duplicate.show()"
      ],
      "metadata": {
        "id": "Sc4i68Bj9tlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42559de-0be4-4df5-cbf0-174158c4c6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+\n",
            "| id|weight|height|age|gender|\n",
            "+---+------+------+---+------+\n",
            "|  2| 167.2|   5.4| 45|     M|\n",
            "|  5| 133.2|   5.7| 54|     F|\n",
            "|  1| 144.5|   5.9| 33|     M|\n",
            "|  5| 129.2|   5.3| 42|     M|\n",
            "|  3| 124.1|   5.2| 23|     F|\n",
            "+---+------+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If two rows have the same ID but different data create a column of new IDs for each person"
      ],
      "metadata": {
        "id": "Ow3m2AUgld8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "df3_with_unique_ids = df3.withColumn(\"new_id\", monotonically_increasing_id())\n",
        "\n",
        "df3_with_unique_ids.show()"
      ],
      "metadata": {
        "id": "HHJBFlKN9xE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe2ae0e-7796-4201-880c-bf49de863794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+------+----------+\n",
            "| id|weight|height|age|gender|    new_id|\n",
            "+---+------+------+---+------+----------+\n",
            "|  1| 144.5|   5.9| 33|     M|         0|\n",
            "|  2| 167.2|   5.4| 45|     M|         1|\n",
            "|  3| 124.1|   5.2| 23|     F|         2|\n",
            "|  4| 144.5|   5.9| 33|     M|8589934592|\n",
            "|  5| 133.2|   5.7| 54|     F|8589934593|\n",
            "|  3| 124.1|   5.2| 23|     F|8589934594|\n",
            "|  5| 129.2|   5.3| 42|     M|8589934595|\n",
            "+---+------+------+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create the following dataframe\n",
        "\n",
        "\n",
        " df_miss = spark.createDataFrame([(1, 143.5, 5.6, 28, 'M', 100000),(2, 167.2, 5.4, 45, 'M', None),(3, None , 5.2, None, None, None),(4, 144.5, 5.9, 33, 'M', None),(5, 133.2, 5.7, 54, 'F', None),\n",
        " (6, 124.1, 5.2, None, 'F', None),(7, 129.2, 5.3, 42, 'M', 76000),], ['id', 'weight', 'height', 'age', 'gender', 'income'])"
      ],
      "metadata": {
        "id": "hlONXqAzlwoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema_miss = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"weight\", DoubleType(), True),\n",
        "    StructField(\"height\", DoubleType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"income\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data_miss = [\n",
        "    (1, 143.5, 5.6, 28, 'M', 100000),\n",
        "    (2, 167.2, 5.4, 45, 'M', None),\n",
        "    (3, None, 5.2, None, None, None),\n",
        "    (4, 144.5, 5.9, 33, 'M', None),\n",
        "    (5, 133.2, 5.7, 54, 'F', None),\n",
        "    (6, 124.1, 5.2, None, 'F', None),\n",
        "    (7, 129.2, 5.3, 42, 'M', 76000)\n",
        "]\n",
        "\n",
        "df_miss = spark.createDataFrame(data_miss, schema=schema_miss)\n",
        "\n",
        "df_miss.show()"
      ],
      "metadata": {
        "id": "QYq0kGQj-HBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23236c2e-51f2-41ee-d428-44c6ff4d5037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+----+------+------+\n",
            "| id|weight|height| age|gender|income|\n",
            "+---+------+------+----+------+------+\n",
            "|  1| 143.5|   5.6|  28|     M|100000|\n",
            "|  2| 167.2|   5.4|  45|     M|  null|\n",
            "|  3|  null|   5.2|null|  null|  null|\n",
            "|  4| 144.5|   5.9|  33|     M|  null|\n",
            "|  5| 133.2|   5.7|  54|     F|  null|\n",
            "|  6| 124.1|   5.2|null|     F|  null|\n",
            "|  7| 129.2|   5.3|  42|     M| 76000|\n",
            "+---+------+------+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "replace missing values with maen of each column"
      ],
      "metadata": {
        "id": "QWpIFw76l-o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, when, col\n",
        "\n",
        "mean_values = df_miss.select([\n",
        "    mean(when(col(column).isNotNull(), col(column))).alias(column)\n",
        "    for column in df_miss.columns\n",
        "])\n",
        "\n",
        "def fill_with_mean(df, mean_values):\n",
        "    for column in df.columns:\n",
        "        mean_value = mean_values.first()[column]\n",
        "        df = df.withColumn(column, when(df[column].isNull(), mean_value).otherwise(df[column]))\n",
        "    return df\n",
        "\n",
        "df_miss_filled = fill_with_mean(df_miss, mean_values)\n",
        "\n",
        "df_miss_filled.show()"
      ],
      "metadata": {
        "id": "L8LEapVA_dt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161ea3cf-3619-42ae-8696-47e090952a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------+----+------+--------+\n",
            "| id|            weight|height| age|gender|  income|\n",
            "+---+------------------+------+----+------+--------+\n",
            "|1.0|             143.5|   5.6|28.0|     M|100000.0|\n",
            "|2.0|             167.2|   5.4|45.0|     M| 88000.0|\n",
            "|3.0|140.28333333333333|   5.2|40.0|  null| 88000.0|\n",
            "|4.0|             144.5|   5.9|33.0|     M| 88000.0|\n",
            "|5.0|             133.2|   5.7|54.0|     F| 88000.0|\n",
            "|6.0|             124.1|   5.2|40.0|     F| 88000.0|\n",
            "|7.0|             129.2|   5.3|42.0|     M| 76000.0|\n",
            "+---+------------------+------+----+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following dataframe:\n",
        "\n",
        "\n",
        " df_outliers = spark.createDataFrame([(1, 143.5, 5.3, 28),(2, 154.2, 5.5, 45),(3, 342.3, 5.1, 99),(4, 144.5, 5.5, 33),(5, 133.2, 5.4, 54),(6, 124.1, 5.1, 21),(7, 129.2, 5.3, 42),], ['id', 'weight', 'height', 'age'])"
      ],
      "metadata": {
        "id": "W9lV_-Q8mCew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import stddev, mean, col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Outliers\").getOrCreate()\n",
        "\n",
        "schema_outliers = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"weight\", DoubleType(), True),\n",
        "    StructField(\"height\", DoubleType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data_outliers = [\n",
        "    (1, 143.5, 5.3, 28),\n",
        "    (2, 154.2, 5.5, 45),\n",
        "    (3, 342.3, 5.1, 99),\n",
        "    (4, 144.5, 5.5, 33),\n",
        "    (5, 133.2, 5.4, 54),\n",
        "    (6, 124.1, 5.1, 21),\n",
        "    (7, 129.2, 5.3, 42)\n",
        "]"
      ],
      "metadata": {
        "id": "l6cz2mTE_iur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point out the outliers in the above dataframe"
      ],
      "metadata": {
        "id": "ULNpSG-5m2P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_outliers = spark.createDataFrame(data_outliers, schema=schema_outliers)\n",
        "\n",
        "mean_weight = df_outliers.select(mean(col('weight'))).collect()[0][0]\n",
        "stddev_weight = df_outliers.select(stddev(col('weight'))).collect()[0][0]\n",
        "\n",
        "mean_height = df_outliers.select(mean(col('height'))).collect()[0][0]\n",
        "stddev_height = df_outliers.select(stddev(col('height'))).collect()[0][0]\n",
        "\n",
        "mean_age = df_outliers.select(mean(col('age'))).collect()[0][0]\n",
        "stddev_age = df_outliers.select(stddev(col('age'))).collect()[0][0]\n",
        "\n",
        "df_outliers = df_outliers.withColumn('weight_zscore', (col('weight') - mean_weight) / stddev_weight)\n",
        "df_outliers = df_outliers.withColumn('height_zscore', (col('height') - mean_height) / stddev_height)\n",
        "df_outliers = df_outliers.withColumn('age_zscore', (col('age') - mean_age) / stddev_age)\n",
        "\n",
        "z_score_threshold = 2000\n",
        "\n",
        "outliers = df_outliers.filter(\n",
        "    (F.abs(col('weight_zscore')) > z_score_threshold) |\n",
        "    (F.abs(col('height_zscore')) > z_score_threshold) |\n",
        "    (F.abs(col('age_zscore')) > z_score_threshold)\n",
        ")\n",
        "\n",
        "outliers.show()"
      ],
      "metadata": {
        "id": "_lBq_E-1_8rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5be9e83-a32b-40b1-f75c-4b4b785ec870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+---+-------------+-------------+----------+\n",
            "| id|weight|height|age|weight_zscore|height_zscore|age_zscore|\n",
            "+---+------+------+---+-------------+-------------+----------+\n",
            "+---+------+------+---+-------------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}